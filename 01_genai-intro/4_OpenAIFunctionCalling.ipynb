{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we'll demonstrate how to use OpenAI functioncalling.  We'll use the GPT-3.5 model to identify and call a function in the context of a conversation. We will use a simple function that returns a string as a response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Importing the necessary library for OpenAI API\n",
    "import openai\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Define your OpenAI API key \n",
    "client = openai.OpenAI(\n",
    "    base_url = \"https://openai.vocareum.com/v1\",\n",
    "    api_key = os.environ.get('API_KEY')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Define the Function\n",
    "\n",
    "First, let's define a simple Python function that takes a string as an argument and returns a string as a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restaurant_localizer(restaurant, location):\n",
    "    return f\"Function called with restaurant {restaurant} from location {location}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Define the Tools\n",
    "\n",
    "Next, we define the `tools` list. This list contains the functions that the assistant can use. Each function is described by its name, description, and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"restaurant_localizer\",\n",
    "            \"description\": \"A simple function that localizes a restaurant\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"restaurant\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The restaurant name\"\n",
    "                    },\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The location of the restaurant\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"restaurant\", \"location\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Define the Initial Conversation Messages\n",
    "\n",
    "We define the initial conversation messages. The first message is from the system, describing the role of the assistant. The second message is from the user, requesting the function call. Feel free to change the argument!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an assistant that localizes restaurants.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Localize the restaurant 'Pizza Place' in 'San Francisco'.\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Generate a Response from the Model\n",
    "\n",
    "We call `openai.ChatCompletion.create()` (for openai < 1.0) or `openai.chat.completions.create()` (for openai > 1.0) to generate a response from the GPT-3.5 model. The model, messages, and tools are passed as arguments. The `tool_choice` is set to \"auto\", allowing the model to choose which tool (function) to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Extract the Tool Calls and Call the Function\n",
    "\n",
    "We extract any tool calls from the response. \n",
    "\n",
    "- If there are any tool calls, we map the tool names to their corresponding Python functions. \n",
    "- For each tool call in the response, we extract the function name and call the corresponding Python function with the arguments provided by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_calls = response.choices[0].message.tool_calls\n",
    "\n",
    "if tool_calls:\n",
    "    available_functions = {\n",
    "        \"restaurant_localizer\": restaurant_localizer,\n",
    "    }\n",
    "\n",
    "    for tool_call in tool_calls:\n",
    "        function_name = tool_call.function.name\n",
    "        function_to_call = available_functions[function_name]\n",
    "        function_args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "        if function_name == 'restaurant_localizer':\n",
    "            function_response = restaurant_localizer(\n",
    "                restaurant=function_args.get(\"restaurant\"),\n",
    "                location=function_args.get(\"location\"),\n",
    "            )\n",
    "\n",
    "        messages.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": function_response,\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Print the Conversation\n",
    "\n",
    "Finally, we print the conversation messages to see the function call and response in the context of the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system: You are an assistant that localizes restaurants.\n",
      "user: Localize the restaurant 'Pizza Place' in 'San Francisco'.\n",
      "assistant: Function called with restaurant Pizza Place from location San Francisco\n",
      "assistant: Function called with restaurant Pizza Place from location San Francisco\n"
     ]
    }
   ],
   "source": [
    "for message in messages:\n",
    "    print(f\"{message['role']}: {message['content']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
